{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274,
          "referenced_widgets": [
            "1e17f0a1c1a0430181b65f5897634c41",
            "fc753480e95a433eb5e08dda444ea760",
            "a6d96fb63ae94287b8d36c1934a74e29",
            "7ae3dd01b2904ec2b558863267f97b93",
            "99e36565df8b40249b75030702fd23e7",
            "e440ccd6cbbb43b8b8c52d3976b0e74c",
            "4060f7a1e89a4b689c6013146f088b18",
            "da5f7b10d55f4257bfab1810a58412ca",
            "af35f637089d46e1a42c9dcae9422503",
            "828a5316430d483e800d1569dd6ff41d",
            "a8f34a152cff4e62807288fb1a4007f7",
            "66e7222406dd4aaca27014ffa02f8b6b",
            "49a15e892c0642c7bb14d15f88871d97",
            "de5994b7a60c4932b1f2c502c11fb013",
            "6a44df16a9e04dc8a9a091a65128d47b",
            "056414beb68744df9bb875616146a0d8",
            "6cebe472cf3b46a181c41036020e5b81"
          ]
        },
        "id": "95Wqo_H6fnTf",
        "outputId": "ae37329b-d7f2-4f52-fceb-febf083a2c33"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e17f0a1c1a0430181b65f5897634c41"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import kagglehub\n",
        "kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0dYQtqlgz7S",
        "outputId": "19479973-5ceb-4b42-da95-17b6964d8f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "nfl_big_data_bowl_2026_prediction_path = kagglehub.competition_download('nfl-big-data-bowl-2026-prediction')\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCP4r_u9WeCa",
        "outputId": "9a551153-a5b4-47ec-8106-c00f7eee162c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to 42\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "import json\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# scikit-learn imports (for data processing only)\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TensorFlow imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Fix all random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    print(f\"Random seed set to {seed}\")\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEfBnYNmflsm",
        "outputId": "1f61ab6f-399e-456a-e7e5-4249fca811a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 18 input files and 18 output files\n",
            "Loading week 1/18...\n",
            "Loading week 2/18...\n",
            "Loading week 3/18...\n",
            "Loading week 4/18...\n",
            "Loading week 5/18...\n",
            "Loading week 6/18...\n",
            "Loading week 7/18...\n",
            "Loading week 8/18...\n",
            "Loading week 9/18...\n",
            "Loading week 10/18...\n",
            "Loading week 11/18...\n",
            "Loading week 12/18...\n",
            "Loading week 13/18...\n",
            "Loading week 14/18...\n",
            "Loading week 15/18...\n",
            "Loading week 16/18...\n",
            "Loading week 17/18...\n",
            "Loading week 18/18...\n",
            "Train input shape: (4880579, 23)\n",
            "Train output shape: (562936, 6)\n",
            "Test input shape: (49753, 23)\n",
            "Test shape: (5837, 5)\n"
          ]
        }
      ],
      "source": [
        "BASE_DIR = nfl_big_data_bowl_2026_prediction_path\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
        "\n",
        "# Load training data\n",
        "input_files = sorted(glob.glob(os.path.join(TRAIN_DIR, 'input_*.csv')))\n",
        "output_files = sorted(glob.glob(os.path.join(TRAIN_DIR, 'output_*.csv')))\n",
        "\n",
        "print(f\"Found {len(input_files)} input files and {len(output_files)} output files\")\n",
        "\n",
        "train_input_list = []\n",
        "train_output_list = []\n",
        "\n",
        "for i, (inp_file, out_file) in enumerate(zip(input_files, output_files)):\n",
        "    print(f\"Loading week {i+1}/{len(input_files)}...\")\n",
        "    inp_df = pd.read_csv(inp_file)\n",
        "    out_df = pd.read_csv(out_file)\n",
        "    train_input_list.append(inp_df)\n",
        "    train_output_list.append(out_df)\n",
        "\n",
        "train_input = pd.concat(train_input_list, ignore_index=True)\n",
        "train_output = pd.concat(train_output_list, ignore_index=True)\n",
        "\n",
        "print(f\"Train input shape: {train_input.shape}\")\n",
        "print(f\"Train output shape: {train_output.shape}\")\n",
        "\n",
        "test_input = pd.read_csv(os.path.join(BASE_DIR, 'test_input.csv'))\n",
        "test = pd.read_csv(os.path.join(BASE_DIR, 'test.csv'))\n",
        "\n",
        "print(f\"Test input shape: {test_input.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_Lm13OCgFga"
      },
      "outputs": [],
      "source": [
        "# Feature engineering functions (same as original)\n",
        "def add_enhanced_features(df):\n",
        "    \"\"\"Enhanced feature engineering with opponent interaction features\"\"\"\n",
        "    df = df.copy()\n",
        "    print(\"Adding enhanced features...\")\n",
        "\n",
        "    # Mirror transform - normalize to right direction\n",
        "    left_mask = df[\"play_direction\"] == \"left\"\n",
        "    df.loc[left_mask, \"x\"] = 120 - df.loc[left_mask, \"x\"]\n",
        "    df.loc[left_mask, \"y\"] = 53.3 - df.loc[left_mask, \"y\"]\n",
        "    df.loc[left_mask, \"ball_land_x\"] = 120 - df.loc[left_mask, \"ball_land_x\"]\n",
        "    df.loc[left_mask, \"ball_land_y\"] = 53.3 - df.loc[left_mask, \"ball_land_y\"]\n",
        "\n",
        "    # Direction encoding (circular features)\n",
        "    df[\"dir_rad\"] = np.deg2rad(df[\"dir\"]) % (2 * np.pi)\n",
        "    df[\"dir_sin\"] = np.sin(df[\"dir_rad\"])\n",
        "    df[\"dir_cos\"] = np.cos(df[\"dir_rad\"])\n",
        "    df[\"o_rad\"] = np.deg2rad(df[\"o\"]) % (2 * np.pi)\n",
        "    df[\"o_sin\"] = np.sin(df[\"o_rad\"])\n",
        "    df[\"o_cos\"] = np.cos(df[\"o_rad\"])\n",
        "\n",
        "    # Velocity components\n",
        "    df[\"vx\"] = df[\"s\"] * np.cos(df[\"dir_rad\"])\n",
        "    df[\"vy\"] = df[\"s\"] * np.sin(df[\"dir_rad\"])\n",
        "\n",
        "    # Acceleration components\n",
        "    df[\"ax\"] = df[\"a\"] * np.cos(df[\"dir_rad\"])\n",
        "    df[\"ay\"] = df[\"a\"] * np.sin(df[\"dir_rad\"])\n",
        "\n",
        "    # Ball landing features\n",
        "    df[\"dx_to_land\"] = df[\"ball_land_x\"] - df[\"x\"]\n",
        "    df[\"dy_to_land\"] = df[\"ball_land_y\"] - df[\"y\"]\n",
        "    df[\"dist_to_land\"] = np.sqrt(df[\"dx_to_land\"]**2 + df[\"dy_to_land\"]**2)\n",
        "\n",
        "    # Position encoding\n",
        "    position_map = {'QB':0, 'RB':1, 'WR':2, 'TE':3, 'FB':4, 'OL':5, 'DL':6, 'LB':7, 'DB':8, 'S':9}\n",
        "    df[\"pos_enc\"] = df[\"player_position\"].map(position_map).fillna(10)\n",
        "\n",
        "    # Target receiver flag\n",
        "    if \"player_to_predict\" in df.columns:\n",
        "        df[\"is_target\"] = df[\"player_to_predict\"]\n",
        "    else:\n",
        "        df[\"is_target\"] = 0\n",
        "\n",
        "    # Sort for temporal and spatial features\n",
        "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
        "    group_cols = ['game_id', 'play_id', 'nfl_id']\n",
        "\n",
        "    # Lag features (1 only)\n",
        "    df['x_lag1'] = df.groupby(group_cols)['x'].shift(1)\n",
        "    df['y_lag1'] = df.groupby(group_cols)['y'].shift(1)\n",
        "    df['s_lag1'] = df.groupby(group_cols)['s'].shift(1)\n",
        "\n",
        "    # Speed change\n",
        "    df['speed_change'] = df.groupby(group_cols)['s'].diff()\n",
        "\n",
        "    # Distance to sidelines\n",
        "    df['dist_from_sideline'] = np.minimum(df['y'], 53.3 - df['y'])\n",
        "\n",
        "    # Opponent/Teammate interaction features\n",
        "    print(\"Computing player interactions...\")\n",
        "    df = add_opponent_features_fast(df)\n",
        "\n",
        "    # Fill NaN values\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    print(\"Enhanced feature engineering complete!\")\n",
        "    return df\n",
        "\n",
        "def add_opponent_features_fast(df):\n",
        "    \"\"\"Add opponent features - optimized version (last frame only)\"\"\"\n",
        "    last_frames = (df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
        "                     .groupby(['game_id', 'play_id', 'nfl_id'])\n",
        "                     .tail(1)\n",
        "                     .copy())\n",
        "\n",
        "    opponent_data = []\n",
        "\n",
        "    for (gid, pid), play_group in tqdm(last_frames.groupby(['game_id', 'play_id']),\n",
        "                                       desc=\"Player interactions\"):\n",
        "        if len(play_group) < 2:\n",
        "            for idx in play_group.index:\n",
        "                opponent_data.append({\n",
        "                    'game_id': gid,\n",
        "                    'play_id': pid,\n",
        "                    'nfl_id': play_group.loc[idx, 'nfl_id'],\n",
        "                    'nearest_teammate_dist': 50.0,\n",
        "                    'nearest_opponent_dist': 50.0,\n",
        "                    'num_nearby_3yd': 0\n",
        "                })\n",
        "            continue\n",
        "\n",
        "        positions = play_group[['x', 'y']].values\n",
        "        sides = play_group['player_side'].values\n",
        "        nfl_ids = play_group['nfl_id'].values\n",
        "\n",
        "        for i in range(len(play_group)):\n",
        "            other_mask = np.arange(len(positions)) != i\n",
        "            other_positions = positions[other_mask]\n",
        "            other_sides = sides[other_mask]\n",
        "\n",
        "            distances = np.sqrt(((positions[i] - other_positions) ** 2).sum(axis=1))\n",
        "\n",
        "            teammate_mask = other_sides == sides[i]\n",
        "            nearest_teammate = distances[teammate_mask].min() if teammate_mask.any() else 50.0\n",
        "\n",
        "            opponent_mask = other_sides != sides[i]\n",
        "            nearest_opponent = distances[opponent_mask].min() if opponent_mask.any() else 50.0\n",
        "\n",
        "            num_nearby = (distances < 3.0).sum()\n",
        "\n",
        "            opponent_data.append({\n",
        "                'game_id': gid,\n",
        "                'play_id': pid,\n",
        "                'nfl_id': nfl_ids[i],\n",
        "                'nearest_teammate_dist': nearest_teammate,\n",
        "                'nearest_opponent_dist': nearest_opponent,\n",
        "                'num_nearby_3yd': int(num_nearby)\n",
        "            })\n",
        "\n",
        "    opponent_df = pd.DataFrame(opponent_data)\n",
        "    df = df.merge(opponent_df, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
        "\n",
        "    df['nearest_teammate_dist'] = df['nearest_teammate_dist'].fillna(50.0)\n",
        "    df['nearest_opponent_dist'] = df['nearest_opponent_dist'].fillna(50.0)\n",
        "    df['num_nearby_3yd'] = df['num_nearby_3yd'].fillna(0)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCODy4UugKUa",
        "outputId": "1c670ee0-ad4b-4aa1-a6e4-9974e9c920d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total features: 26\n"
          ]
        }
      ],
      "source": [
        "# Define features - 26 features\n",
        "FEATURES = [\n",
        "    'x', 'y', 'vx', 'vy', 's', 'a', 'ax', 'ay',\n",
        "    'dir_sin', 'dir_cos', 'o_sin', 'o_cos',\n",
        "    'dx_to_land', 'dy_to_land', 'dist_to_land',\n",
        "    'pos_enc', 'is_target', 'absolute_yardline_number',\n",
        "    'x_lag1', 'y_lag1', 's_lag1',\n",
        "    'speed_change',\n",
        "    'dist_from_sideline',\n",
        "    'nearest_teammate_dist', 'nearest_opponent_dist', 'num_nearby_3yd'\n",
        "]\n",
        "\n",
        "print(f\"Total features: {len(FEATURES)}\")\n",
        "\n",
        "def prepare_data_efficient(input_df, output_df, max_inp=60, max_out=40):\n",
        "    \"\"\"Prepare forecasting data: Input -> Output\"\"\"\n",
        "    print(\"Preparing forecasting data...\")\n",
        "\n",
        "    input_plays = input_df.groupby(['game_id', 'play_id'])\n",
        "    output_plays = output_df.groupby(['game_id', 'play_id'])\n",
        "\n",
        "    inp_seqs, out_seqs, inp_masks, out_masks = [], [], [], []\n",
        "\n",
        "    for (gid, pid), inp_group in tqdm(input_plays, desc=\"Processing\"):\n",
        "        try:\n",
        "            out_group = output_plays.get_group((gid, pid))\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        for nid in inp_group['nfl_id'].unique():\n",
        "            inp_player = inp_group[inp_group['nfl_id'] == nid].sort_values('frame_id')\n",
        "            out_player = out_group[out_group['nfl_id'] == nid].sort_values('frame_id')\n",
        "\n",
        "            if len(out_player) == 0:\n",
        "                continue\n",
        "\n",
        "            inp_feat = inp_player[FEATURES].values\n",
        "            out_pos = out_player[['x', 'y']].values\n",
        "\n",
        "            # Pad/truncate input\n",
        "            if len(inp_feat) > max_inp:\n",
        "                inp_feat = inp_feat[-max_inp:]\n",
        "                inp_mask = np.ones(max_inp)\n",
        "            else:\n",
        "                pad = max_inp - len(inp_feat)\n",
        "                inp_feat = np.vstack([inp_feat, np.zeros((pad, len(FEATURES)))])\n",
        "                inp_mask = np.concatenate([np.ones(len(inp_player)), np.zeros(pad)])\n",
        "\n",
        "            # Pad/truncate output\n",
        "            if len(out_pos) > max_out:\n",
        "                out_pos = out_pos[:max_out]\n",
        "                out_mask = np.ones(max_out)\n",
        "            else:\n",
        "                pad = max_out - len(out_pos)\n",
        "                out_pos = np.vstack([out_pos, np.zeros((pad, 2))])\n",
        "                out_mask = np.concatenate([np.ones(len(out_player)), np.zeros(pad)])\n",
        "\n",
        "            inp_seqs.append(inp_feat)\n",
        "            out_seqs.append(out_pos)\n",
        "            inp_masks.append(inp_mask)\n",
        "            out_masks.append(out_mask)\n",
        "\n",
        "    print(f\"Created {len(inp_seqs)} sequences\")\n",
        "    return (np.array(inp_seqs, dtype='float32'),\n",
        "            np.array(out_seqs, dtype='float32'),\n",
        "            np.array(inp_masks, dtype='float32'),\n",
        "            np.array(out_masks, dtype='float32'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kih3HbH-gOxb"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# PYTORCH MODEL (Original)\n",
        "# =========================================================\n",
        "class LightweightForecaster(nn.Module):\n",
        "    \"\"\"Encoder-Decoder for position forecasting\"\"\"\n",
        "    def __init__(self, input_dim=26, hidden_dim=256, max_out=40):\n",
        "        super().__init__()\n",
        "        self.max_out = max_out\n",
        "\n",
        "        # Encoder\n",
        "        self.enc_embed = nn.Linear(input_dim, hidden_dim)\n",
        "        self.enc_gru = nn.GRU(hidden_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.0)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec_embed = nn.Linear(2, hidden_dim)\n",
        "        self.dec_gru = nn.GRU(hidden_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.0)\n",
        "\n",
        "        # Output\n",
        "        self.out_proj = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    def forward(self, x, target=None):\n",
        "        B = x.size(0)\n",
        "\n",
        "        # Encode\n",
        "        enc = F.relu(self.enc_embed(x))\n",
        "        _, h = self.enc_gru(enc)\n",
        "\n",
        "        if self.training and target is not None:\n",
        "            # Teacher forcing\n",
        "            dec_inp = torch.cat([torch.zeros(B, 1, 2, device=x.device), target[:, :-1]], dim=1)\n",
        "            dec = F.relu(self.dec_embed(dec_inp))\n",
        "            dec_out, _ = self.dec_gru(dec, h)\n",
        "            return self.out_proj(dec_out)\n",
        "        else:\n",
        "            # Autoregressive\n",
        "            outputs = []\n",
        "            prev = torch.zeros(B, 1, 2, device=x.device)\n",
        "            h_dec = h\n",
        "\n",
        "            for _ in range(self.max_out):\n",
        "                dec = F.relu(self.dec_embed(prev))\n",
        "                dec_out, h_dec = self.dec_gru(dec, h_dec)\n",
        "                pos = self.out_proj(dec_out)\n",
        "                outputs.append(pos)\n",
        "                prev = pos\n",
        "\n",
        "            return torch.cat(outputs, dim=1)\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, inp, out, inp_mask, out_mask):\n",
        "        self.inp = torch.from_numpy(inp)\n",
        "        self.out = torch.from_numpy(out)\n",
        "        self.inp_mask = torch.from_numpy(inp_mask)\n",
        "        self.out_mask = torch.from_numpy(out_mask)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inp)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.inp[i], self.out[i], self.inp_mask[i], self.out_mask[i]\n",
        "\n",
        "def train_pytorch_model(model, train_loader, val_loader, epochs=20):\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4,\n",
        "                                   betas=(0.9, 0.999), eps=1e-8)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    best_rmse = float('inf')\n",
        "    patience = 8\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for inp, out, _, out_mask in tqdm(train_loader, desc=f\"PyTorch Epoch {epoch+1}/{epochs}\"):\n",
        "            inp, out, out_mask = inp.to(device), out.to(device), out_mask.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(inp, target=out)\n",
        "\n",
        "            loss = criterion(pred, out).sum(-1) * out_mask\n",
        "            loss = loss.sum() / (out_mask.sum() + 1e-6)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * len(inp)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds, val_tgts, val_masks = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inp, out, _, out_mask in val_loader:\n",
        "                inp = inp.to(device)\n",
        "                pred = model(inp)\n",
        "                val_preds.append(pred.cpu())\n",
        "                val_tgts.append(out)\n",
        "                val_masks.append(out_mask)\n",
        "\n",
        "        val_preds = torch.cat(val_preds)\n",
        "        val_tgts = torch.cat(val_tgts)\n",
        "        val_masks = torch.cat(val_masks)\n",
        "\n",
        "        mask_exp = val_masks.unsqueeze(-1).expand_as(val_preds) > 0\n",
        "        rmse = torch.sqrt(((val_preds[mask_exp] - val_tgts[mask_exp])**2).mean()).item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02d} | Loss: {train_loss:.4f} | RMSE: {rmse:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            counter = 0\n",
        "            torch.save(model.state_dict(), \"best_model_pytorch.pt\")\n",
        "            print(f\"Saved! Best: {rmse:.4f}\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    return best_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdb4ynBogOuo"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# TENSORFLOW MODEL (Matching PyTorch structure)\n",
        "# =========================================================\n",
        "class LightweightForecasterTF(keras.Model):\n",
        "    \"\"\"TensorFlow model matching PyTorch LightweightForecaster exactly\"\"\"\n",
        "    def __init__(self, input_dim=26, hidden_dim=256, max_out=40):\n",
        "        super().__init__()\n",
        "        self.max_out = max_out\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.enc_embed = layers.Dense(hidden_dim, activation='relu', name='enc_embed')\n",
        "        # 2-layer stacked GRU using GRUCell (matching PyTorch num_layers=2)\n",
        "        self.enc_gru_cell1 = layers.GRUCell(hidden_dim, dropout=0.0, recurrent_dropout=0.0, name='enc_gru_cell1')\n",
        "        self.enc_gru_cell2 = layers.GRUCell(hidden_dim, dropout=0.0, recurrent_dropout=0.0, name='enc_gru_cell2')\n",
        "        self.enc_gru = layers.RNN(layers.StackedRNNCells([self.enc_gru_cell1, self.enc_gru_cell2]),\n",
        "                                   return_sequences=False, return_state=True, name='enc_gru')\n",
        "\n",
        "        # Decoder\n",
        "        self.dec_embed = layers.Dense(hidden_dim, activation='relu', name='dec_embed')\n",
        "        # 2-layer stacked GRU for decoder\n",
        "        self.dec_gru_cell1 = layers.GRUCell(hidden_dim, dropout=0.0, recurrent_dropout=0.0, name='dec_gru_cell1')\n",
        "        self.dec_gru_cell2 = layers.GRUCell(hidden_dim, dropout=0.0, recurrent_dropout=0.0, name='dec_gru_cell2')\n",
        "        self.dec_gru = layers.RNN(layers.StackedRNNCells([self.dec_gru_cell1, self.dec_gru_cell2]),\n",
        "                                   return_sequences=True, return_state=True, name='dec_gru')\n",
        "\n",
        "        # Output\n",
        "        self.out_proj = layers.Dense(2, name='out_proj')\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"Forward pass - supports both training (teacher forcing) and inference (autoregressive)\"\"\"\n",
        "        if isinstance(inputs, (list, tuple)) and len(inputs) == 2:\n",
        "            # Training mode: [encoder_input, decoder_targets]\n",
        "            encoder_input, decoder_targets = inputs\n",
        "            return self._forward_teacher_forcing(encoder_input, decoder_targets, training)\n",
        "        else:\n",
        "            # Inference mode: encoder_input only\n",
        "            encoder_input = inputs\n",
        "            return self._forward_autoregressive(encoder_input, training)\n",
        "\n",
        "    def _forward_teacher_forcing(self, encoder_input, decoder_targets, training):\n",
        "        \"\"\"Teacher forcing for training\"\"\"\n",
        "        # Encode - StackedRNNCells returns 2 states (one per layer)\n",
        "        enc = self.enc_embed(encoder_input)\n",
        "        _, h1_enc, h2_enc = self.enc_gru(enc, training=training)\n",
        "        init_states_dec = [h1_enc, h2_enc]\n",
        "\n",
        "        # Shift decoder targets for teacher forcing (prepend zeros)\n",
        "        batch_size = tf.shape(decoder_targets)[0]\n",
        "        dtype = decoder_targets.dtype\n",
        "        shifted_targets = tf.concat([\n",
        "            tf.zeros((batch_size, 1, 2), dtype=dtype),\n",
        "            decoder_targets[:, :-1, :]\n",
        "        ], axis=1)\n",
        "\n",
        "        # Decode with teacher forcing - StackedRNNCells returns 2 states\n",
        "        dec = self.dec_embed(shifted_targets)\n",
        "        dec_out, _, _ = self.dec_gru(dec, initial_state=init_states_dec, training=training)\n",
        "        output = self.out_proj(dec_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _forward_autoregressive(self, encoder_input, training):\n",
        "        \"\"\"Autoregressive inference\"\"\"\n",
        "        # Encode - StackedRNNCells returns 2 states (one per layer)\n",
        "        enc = self.enc_embed(encoder_input)\n",
        "        _, h1_dec, h2_dec = self.enc_gru(enc, training=training)\n",
        "\n",
        "        # Autoregressive decoding\n",
        "        batch_size = tf.shape(encoder_input)[0]\n",
        "        dtype = encoder_input.dtype\n",
        "        prev_pos = tf.zeros((batch_size, 1, 2), dtype=dtype)\n",
        "        outputs = []\n",
        "\n",
        "        for _ in range(self.max_out):\n",
        "            dec = self.dec_embed(prev_pos)\n",
        "            # StackedRNNCells returns 2 states, need to pass list and unpack list\n",
        "            dec_out, h1_dec, h2_dec = self.dec_gru(dec, initial_state=[h1_dec, h2_dec], training=training)\n",
        "            pos = self.out_proj(dec_out)\n",
        "            outputs.append(pos)\n",
        "            prev_pos = pos\n",
        "\n",
        "        return tf.concat(outputs, axis=1)\n",
        "\n",
        "    def predict_autoregressive(self, encoder_input):\n",
        "        \"\"\"Explicit autoregressive prediction method\"\"\"\n",
        "        result = self._forward_autoregressive(encoder_input, training=False)\n",
        "        # Convert to numpy for consistency\n",
        "        if isinstance(result, tf.Tensor):\n",
        "            return result.numpy()\n",
        "        return result\n",
        "\n",
        "def create_tf_model_simple(input_dim=26, hidden_dim=256, max_out=40):\n",
        "    \"\"\"Create TensorFlow model matching PyTorch structure exactly\"\"\"\n",
        "    model = LightweightForecasterTF(input_dim=input_dim, hidden_dim=hidden_dim, max_out=max_out)\n",
        "    return model\n",
        "\n",
        "def mse_per_timestep(y_true, y_pred):\n",
        "    \"\"\"MSE per timestep - returns (batch, seq_len) for sample_weight to apply mask\"\"\"\n",
        "    # (batch, seq_len, 2) -> (batch, seq_len)\n",
        "    return tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)\n",
        "\n",
        "class MaskedMSEMetric(tf.keras.metrics.Metric):\n",
        "    \"\"\"Masked MSE metric for validation\"\"\"\n",
        "    def __init__(self, name='masked_mse', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.total_mse = self.add_weight(name='total_mse', initializer='zeros')\n",
        "        self.total_count = self.add_weight(name='total_count', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        mse = tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)\n",
        "        if sample_weight is not None:\n",
        "            mse = mse * sample_weight\n",
        "            self.total_mse.assign_add(tf.reduce_sum(mse))\n",
        "            self.total_count.assign_add(tf.reduce_sum(sample_weight))\n",
        "        else:\n",
        "            self.total_mse.assign_add(tf.reduce_sum(mse))\n",
        "            self.total_count.assign_add(tf.cast(tf.size(mse), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.total_mse / (self.total_count + 1e-6)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.total_mse.assign(0.0)\n",
        "        self.total_count.assign(0.0)\n",
        "\n",
        "def train_tensorflow_model(model, X_train, y_train, X_val, y_val,\n",
        "                          mask_train, mask_val, epochs=20):\n",
        "    \"\"\"Train TensorFlow model with masked loss\"\"\"\n",
        "    print(\"Training TensorFlow model...\")\n",
        "\n",
        "    # Compile with per-timestep loss (sample_weight will apply mask)\n",
        "    model.compile(\n",
        "        optimizer=optimizers.AdamW(learning_rate=5e-4, weight_decay=1e-4,\n",
        "                                    beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "        loss=mse_per_timestep,\n",
        "        metrics=[MaskedMSEMetric()]\n",
        "    )\n",
        "\n",
        "    # Cosine annealing schedule matching PyTorch (T_max=epochs, alpha=0.0)\n",
        "    def cosine_annealing_schedule(epoch, lr):\n",
        "        \"\"\"Cosine annealing matching PyTorch CosineAnnealingLR\"\"\"\n",
        "        T_max = epochs\n",
        "        return 5e-4 * (1 + np.cos(np.pi * epoch / T_max)) / 2\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks_list = [\n",
        "        callbacks.EarlyStopping(monitor='val_masked_mse', mode='min', patience=8, restore_best_weights=True),\n",
        "        callbacks.LearningRateScheduler(cosine_annealing_schedule, verbose=0),\n",
        "        callbacks.ModelCheckpoint('best_model_tensorflow.weights.h5',\n",
        "                                 save_best_only=True,\n",
        "                                 monitor='val_masked_mse',\n",
        "                                 mode='min',\n",
        "                                 save_weights_only=True)\n",
        "    ]\n",
        "\n",
        "    # Train with sample weights (masks)\n",
        "    history = model.fit(\n",
        "        [X_train, y_train], y_train,\n",
        "        sample_weight=mask_train,\n",
        "        validation_data=([X_val, y_val], y_val, mask_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks_list,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate with autoregressive inference and mask (matching PyTorch)\n",
        "    val_pred = model.predict_autoregressive(X_val)\n",
        "\n",
        "    # Apply mask for RMSE calculation (matching PyTorch exactly)\n",
        "    # Expand mask from (batch, seq) to (batch, seq, 2) to match predictions\n",
        "    mask_expanded = np.expand_dims(mask_val, axis=-1)  # (batch, seq, 1)\n",
        "    mask_expanded = np.broadcast_to(mask_expanded, val_pred.shape)  # (batch, seq, 2)\n",
        "    mask_bool = mask_expanded > 0\n",
        "    val_pred_masked = val_pred[mask_bool]\n",
        "    y_val_masked = y_val[mask_bool]\n",
        "\n",
        "    rmse = np.sqrt(np.mean((val_pred_masked - y_val_masked)**2))\n",
        "    print(f\"TensorFlow Validation RMSE (masked): {rmse:.4f}\")\n",
        "\n",
        "    return model, rmse, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fWHkMj-gOrj",
        "outputId": "ecd24681-c552-497a-ba1e-69bf4023c2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TRAINING PIPELINE\n",
            "================================================================================\n",
            "Feature Engineering...\n",
            "Adding enhanced features...\n",
            "Computing player interactions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Player interactions: 100%|██████████| 14108/14108 [00:14<00:00, 1001.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced feature engineering complete!\n",
            "Adding enhanced features...\n",
            "Computing player interactions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Player interactions: 100%|██████████| 143/143 [00:00<00:00, 1048.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced feature engineering complete!\n",
            "Train/Val Split...\n",
            "Train samples: 3904237, Val samples: 976342\n",
            "Preparing Data...\n",
            "Preparing forecasting data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 11286/11286 [03:07<00:00, 60.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 36775 sequences\n",
            "Preparing forecasting data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 2822/2822 [00:46<00:00, 60.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 9270 sequences\n",
            "Train shape: (36775, 60, 26), Val shape: (9270, 60, 26)\n",
            "Scaling...\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# MAIN TRAINING PIPELINE\n",
        "# =========================================================\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Feature engineering\n",
        "print(\"Feature Engineering...\")\n",
        "train_input_enh = add_enhanced_features(train_input)\n",
        "test_input_enh = add_enhanced_features(test_input)\n",
        "\n",
        "# Train/Val split\n",
        "print(\"Train/Val Split...\")\n",
        "play_keys = train_input[[\"game_id\",\"play_id\"]].drop_duplicates()\n",
        "train_plays, val_plays = train_test_split(play_keys, test_size=0.2, random_state=SEED)\n",
        "\n",
        "train_idx = train_input_enh.set_index([\"game_id\",\"play_id\"]).index.isin(\n",
        "    train_plays.set_index([\"game_id\",\"play_id\"]).index\n",
        ")\n",
        "val_idx = train_input_enh.set_index([\"game_id\",\"play_id\"]).index.isin(\n",
        "    val_plays.set_index([\"game_id\",\"play_id\"]).index\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {train_idx.sum()}, Val samples: {val_idx.sum()}\")\n",
        "\n",
        "# Prepare data\n",
        "print(\"Preparing Data...\")\n",
        "inp_train, out_train, mask_inp_train, mask_out_train = prepare_data_efficient(\n",
        "    train_input_enh.loc[train_idx],\n",
        "    train_output[train_output.set_index(['game_id','play_id']).index.isin(\n",
        "        train_input_enh.loc[train_idx].set_index(['game_id','play_id']).index\n",
        "    )],\n",
        "    max_inp=60, max_out=40\n",
        ")\n",
        "gc.collect()\n",
        "\n",
        "inp_val, out_val, mask_inp_val, mask_out_val = prepare_data_efficient(\n",
        "    train_input_enh.loc[val_idx],\n",
        "    train_output[train_output.set_index(['game_id','play_id']).index.isin(\n",
        "        train_input_enh.loc[val_idx].set_index(['game_id','play_id']).index\n",
        "    )],\n",
        "    max_inp=60, max_out=40\n",
        ")\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Train shape: {inp_train.shape}, Val shape: {inp_val.shape}\")\n",
        "\n",
        "# Scaling\n",
        "print(\"Scaling...\")\n",
        "scaler = RobustScaler()\n",
        "inp_train_flat = inp_train.reshape(-1, len(FEATURES))\n",
        "scaler.fit(inp_train_flat)\n",
        "inp_train_scaled = scaler.transform(inp_train_flat).reshape(inp_train.shape)\n",
        "inp_val_scaled = scaler.transform(inp_val.reshape(-1, len(FEATURES))).reshape(inp_val.shape)\n",
        "del inp_train_flat\n",
        "gc.collect()\n",
        "\n",
        "# Save scaler\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4rsAR3ZgOmy",
        "outputId": "20686a5e-c785-4381-a78d-ffdb56b90310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING PYTORCH MODEL\n",
            "================================================================================\n",
            "PyTorch model parameters: 1,587,202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 1/20: 100%|██████████| 575/575 [00:06<00:00, 84.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Loss: 1415.5933 | RMSE: 13.6242\n",
            "Saved! Best: 13.6242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 2/20: 100%|██████████| 575/575 [00:06<00:00, 84.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | Loss: 118.3230 | RMSE: 7.0687\n",
            "Saved! Best: 7.0687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 3/20: 100%|██████████| 575/575 [00:06<00:00, 83.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | Loss: 28.6519 | RMSE: 5.4056\n",
            "Saved! Best: 5.4056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 4/20: 100%|██████████| 575/575 [00:06<00:00, 83.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | Loss: 11.5428 | RMSE: 5.0979\n",
            "Saved! Best: 5.0979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 5/20: 100%|██████████| 575/575 [00:06<00:00, 83.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | Loss: 6.3193 | RMSE: 8.3377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 6/20: 100%|██████████| 575/575 [00:06<00:00, 84.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 | Loss: 4.1785 | RMSE: 3.8502\n",
            "Saved! Best: 3.8502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 7/20: 100%|██████████| 575/575 [00:06<00:00, 83.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 | Loss: 2.7939 | RMSE: 3.4562\n",
            "Saved! Best: 3.4562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 8/20: 100%|██████████| 575/575 [00:06<00:00, 84.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | Loss: 1.8743 | RMSE: 4.0200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 9/20: 100%|██████████| 575/575 [00:06<00:00, 84.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | Loss: 1.3589 | RMSE: 3.1265\n",
            "Saved! Best: 3.1265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 10/20: 100%|██████████| 575/575 [00:06<00:00, 84.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Loss: 1.0847 | RMSE: 3.0968\n",
            "Saved! Best: 3.0968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 11/20: 100%|██████████| 575/575 [00:06<00:00, 84.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Loss: 0.8263 | RMSE: 3.7200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 12/20: 100%|██████████| 575/575 [00:06<00:00, 84.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Loss: 0.7267 | RMSE: 2.6805\n",
            "Saved! Best: 2.6805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 13/20: 100%|██████████| 575/575 [00:06<00:00, 84.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Loss: 0.5914 | RMSE: 3.9806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 14/20: 100%|██████████| 575/575 [00:06<00:00, 84.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Loss: 0.4125 | RMSE: 3.1966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 15/20: 100%|██████████| 575/575 [00:06<00:00, 83.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Loss: 0.3350 | RMSE: 2.4392\n",
            "Saved! Best: 2.4392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 16/20: 100%|██████████| 575/575 [00:06<00:00, 84.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Loss: 0.2541 | RMSE: 1.9678\n",
            "Saved! Best: 1.9678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 17/20: 100%|██████████| 575/575 [00:06<00:00, 83.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Loss: 0.2037 | RMSE: 2.0218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 18/20: 100%|██████████| 575/575 [00:06<00:00, 84.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Loss: 0.1630 | RMSE: 1.7133\n",
            "Saved! Best: 1.7133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 19/20: 100%|██████████| 575/575 [00:06<00:00, 83.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Loss: 0.1432 | RMSE: 1.6238\n",
            "Saved! Best: 1.6238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch Epoch 20/20: 100%|██████████| 575/575 [00:06<00:00, 85.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Loss: 0.1323 | RMSE: 1.5523\n",
            "Saved! Best: 1.5523\n",
            "PyTorch Best RMSE: 1.5523\n",
            "\n",
            "================================================================================\n",
            "TRAINING TENSORFLOW MODEL\n",
            "================================================================================\n",
            "TensorFlow model parameters: 1,587,202\n",
            "Training TensorFlow model...\n",
            "Epoch 1/20\n",
            "\u001b[1m575/575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - loss: 566.5207 - masked_mse: 451.7614 - val_loss: 106.1616 - val_masked_mse: 250.9043 - learning_rate: 5.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m575/575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - loss: 63.6776 - masked_mse: 305.1059 - val_loss: 21.8395 - val_masked_mse: 419.9876 - learning_rate: 4.9692e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m575/575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - loss: 14.7428 - masked_mse: 436.2543 - val_loss: 7.2327 - val_masked_mse: 472.7694 - learning_rate: 4.8776e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m575/575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - loss: 5.2497 - masked_mse: 482.4986 - val_loss: 2.9604 - val_masked_mse: 493.5779 - learning_rate: 4.7275e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m575/575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - loss: 2.3341 - masked_mse: 498.5843 - val_loss: 1.4633 - val_masked_mse: 532.1148 - learning_rate: 4.5225e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m575/575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - loss: 1.2506 - masked_mse: 535.2787 - val_loss: 0.8725 - val_masked_mse: 578.6627 - learning_rate: 4.2678e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m575/575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - loss: 0.7698 - masked_mse: 585.0987 - val_loss: 0.6235 - val_masked_mse: 860.2314 - learning_rate: 3.9695e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m575/575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - loss: 0.5446 - masked_mse: 695.3381 - val_loss: 0.4405 - val_masked_mse: 1120.1960 - learning_rate: 3.6350e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m575/575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - loss: 0.4114 - masked_mse: 944.0876 - val_loss: 0.4242 - val_masked_mse: 1156.4326 - learning_rate: 3.2725e-04\n",
            "TensorFlow Validation RMSE (masked): 19.4662\n",
            "TensorFlow Best RMSE: 19.4662\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# TRAIN MODELS\n",
        "# =========================================================\n",
        "results = {}\n",
        "\n",
        "# 1. PyTorch Model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING PYTORCH MODEL\")\n",
        "print(\"=\"*80)\n",
        "train_loader = DataLoader(\n",
        "    SimpleDataset(inp_train_scaled, out_train, mask_inp_train, mask_out_train),\n",
        "    batch_size=64, shuffle=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    SimpleDataset(inp_val_scaled, out_val, mask_inp_val, mask_out_val),\n",
        "    batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "pytorch_model = LightweightForecaster(input_dim=len(FEATURES), hidden_dim=256, max_out=40).to(device)\n",
        "print(f\"PyTorch model parameters: {sum(p.numel() for p in pytorch_model.parameters()):,}\")\n",
        "\n",
        "pytorch_rmse = train_pytorch_model(pytorch_model, train_loader, val_loader, epochs=20)\n",
        "results['PyTorch'] = float(pytorch_rmse)  # Convert to Python float for JSON serialization\n",
        "print(f\"PyTorch Best RMSE: {pytorch_rmse:.4f}\")\n",
        "\n",
        "# 2. TensorFlow Model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING TENSORFLOW MODEL\")\n",
        "print(\"=\"*80)\n",
        "tf_model = create_tf_model_simple(input_dim=len(FEATURES), hidden_dim=256, max_out=40)\n",
        "# Build model before counting parameters (subclassed model needs build)\n",
        "_ = tf_model([np.zeros((1, 60, len(FEATURES)), dtype='float32'),\n",
        "              np.zeros((1, 40, 2), dtype='float32')], training=True)\n",
        "print(f\"TensorFlow model parameters: {tf_model.count_params():,}\")\n",
        "\n",
        "tf_model, tf_rmse, tf_history = train_tensorflow_model(\n",
        "    tf_model, inp_train_scaled, out_train, inp_val_scaled, out_val,\n",
        "    mask_train=mask_out_train, mask_val=mask_out_val, epochs=20\n",
        ")\n",
        "results['TensorFlow'] = float(tf_rmse)  # Convert to Python float for JSON serialization\n",
        "print(f\"TensorFlow Best RMSE: {tf_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBK66cDvgjsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4a9f87-8547-4af5-d6f1-f4a754348bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STATISTICAL COMPARISON\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPARISON RESULTS\n",
            "================================================================================\n",
            "Model                RMSE           \n",
            "-----------------------------------\n",
            "PyTorch              1.5523         \n",
            "TensorFlow           19.4662        \n",
            "\n",
            "Best Model: PyTorch (RMSE: 1.5523)\n",
            "\n",
            "================================================================================\n",
            "STATISTICAL ANALYSIS\n",
            "================================================================================\n",
            "Note: For full statistical comparison, multiple runs would be needed.\n",
            "Current comparison based on single validation set:\n",
            "\n",
            "PyTorch: 1.5523 (baseline - best model)\n",
            "TensorFlow: 19.4662 (+1154.04% better than PyTorch)\n",
            "\n",
            "Results saved to model_comparison_results.json\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# STATISTICAL COMPARISON\n",
        "# =========================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATISTICAL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate detailed metrics for each model\n",
        "comparison_results = {}\n",
        "\n",
        "for model_name, rmse in results.items():\n",
        "    comparison_results[model_name] = {\n",
        "        'RMSE': float(rmse),  # Ensure float type\n",
        "        'RMSE_std': 0.0,  # Would need multiple runs for std\n",
        "    }\n",
        "\n",
        "# Print comparison table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Model':<20} {'RMSE':<15}\")\n",
        "print(\"-\" * 35)\n",
        "for model_name, metrics in comparison_results.items():\n",
        "    print(f\"{model_name:<20} {metrics['RMSE']:<15.4f}\")\n",
        "\n",
        "# Find best model\n",
        "best_model = min(results, key=results.get)\n",
        "best_rmse = float(results[best_model])\n",
        "print(f\"\\nBest Model: {best_model} (RMSE: {best_rmse:.4f})\")\n",
        "\n",
        "# Statistical tests (if we had multiple runs)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATISTICAL ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(\"Note: For full statistical comparison, multiple runs would be needed.\")\n",
        "print(\"Current comparison based on single validation set:\\n\")\n",
        "\n",
        "# Calculate improvement percentages relative to best model\n",
        "for model_name, rmse in results.items():\n",
        "    rmse_float = float(rmse)\n",
        "    if model_name == best_model:\n",
        "        print(f\"{model_name}: {rmse_float:.4f} (baseline - best model)\")\n",
        "    else:\n",
        "        improvement = ((best_rmse - rmse_float) / best_rmse) * 100\n",
        "        if improvement > 0:\n",
        "            print(f\"{model_name}: {rmse_float:.4f} ({improvement:+.2f}% worse than {best_model})\")\n",
        "        else:\n",
        "            print(f\"{model_name}: {rmse_float:.4f} ({abs(improvement):+.2f}% better than {best_model})\")\n",
        "\n",
        "# Save results (ensure all values are JSON serializable)\n",
        "results_json = {k: float(v) for k, v in results.items()}\n",
        "with open('model_comparison_results.json', 'w') as f:\n",
        "    json.dump(results_json, f, indent=2)\n",
        "print(\"\\nResults saved to model_comparison_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW9cK7Dtgluz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "703f0fb8-b998-41b7-ccd3-c4e68bf0e53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "GENERATING SUBMISSIONS\n",
            "================================================================================\n",
            "\n",
            "Generating submissions for all models...\n",
            "Generating PyTorch submission...\n",
            "Adding enhanced features...\n",
            "Computing player interactions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Player interactions: 100%|██████████| 143/143 [00:00<00:00, 1010.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced feature engineering complete!\n",
            "PyTorch submission saved! Shape: (5837, 2)\n",
            "Generating TensorFlow submission...\n",
            "Adding enhanced features...\n",
            "Computing player interactions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Player interactions: 100%|██████████| 143/143 [00:00<00:00, 1034.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced feature engineering complete!\n",
            "TensorFlow submission saved! Shape: (5837, 2)\n",
            "\n",
            "================================================================================\n",
            "ALL SUBMISSIONS GENERATED\n",
            "================================================================================\n",
            "Files created:\n",
            "  - submission_pytorch.csv\n",
            "  - submission_tensorflow.csv\n",
            "  - model_comparison_results.json\n",
            "\n",
            "================================================================================\n",
            "COMPARISON SUMMARY\n",
            "================================================================================\n",
            "Model                RMSE            Submission File               \n",
            "-----------------------------------------------------------------\n",
            "PyTorch              1.5523          submission_pytorch.csv        \n",
            "TensorFlow           19.4662         submission_tensorflow.csv     \n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# GENERATE SUBMISSIONS\n",
        "# =========================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING SUBMISSIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def prepare_test_sequences(test_input_df, max_inp=60):\n",
        "    \"\"\"Prepare test input sequences\"\"\"\n",
        "    test_with_features = add_enhanced_features(test_input_df)\n",
        "    grouped = test_with_features.groupby(['game_id', 'play_id', 'nfl_id'])\n",
        "\n",
        "    sequences = []\n",
        "    metadata = []\n",
        "\n",
        "    for (gid, pid, nid), group in grouped:\n",
        "        group = group.sort_values('frame_id')\n",
        "        inp_feat = group[FEATURES].values\n",
        "\n",
        "        if len(inp_feat) > max_inp:\n",
        "            inp_feat = inp_feat[-max_inp:]\n",
        "        else:\n",
        "            pad = max_inp - len(inp_feat)\n",
        "            inp_feat = np.vstack([inp_feat, np.zeros((pad, len(FEATURES)))])\n",
        "\n",
        "        sequences.append(inp_feat)\n",
        "        metadata.append({'game_id': gid, 'play_id': pid, 'nfl_id': nid})\n",
        "\n",
        "    sequences = np.array(sequences, dtype='float32')\n",
        "    sequences = scaler.transform(sequences.reshape(-1, len(FEATURES))).reshape(sequences.shape)\n",
        "\n",
        "    return sequences, metadata\n",
        "\n",
        "def generate_submission_pytorch(model, test, test_input):\n",
        "    \"\"\"Generate submission for PyTorch model\"\"\"\n",
        "    print(\"Generating PyTorch submission...\")\n",
        "    sequences, metadata = prepare_test_sequences(test_input, max_inp=60)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sequences_tensor = torch.from_numpy(sequences).to(device)\n",
        "        predictions_all = model(sequences_tensor).cpu().numpy()\n",
        "\n",
        "    pred_map = {}\n",
        "    for meta, pred in zip(metadata, predictions_all):\n",
        "        key = (meta['game_id'], meta['play_id'], meta['nfl_id'])\n",
        "        pred_map[key] = pred\n",
        "\n",
        "    results = []\n",
        "    for _, row in test.iterrows():\n",
        "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
        "        frame_id = row['frame_id']\n",
        "\n",
        "        if key in pred_map:\n",
        "            frame_idx = frame_id - 1\n",
        "            if 0 <= frame_idx < 40:\n",
        "                x_pred, y_pred = pred_map[key][frame_idx]\n",
        "            else:\n",
        "                x_pred, y_pred = pred_map[key][-1]\n",
        "        else:\n",
        "            x_pred, y_pred = 60.0, 26.65\n",
        "\n",
        "        x_pred = np.clip(x_pred, 0.0, 120.0)\n",
        "        y_pred = np.clip(y_pred, 0.0, 53.3)\n",
        "        results.append({'x': float(x_pred), 'y': float(y_pred)})\n",
        "\n",
        "    submission = pd.DataFrame(results)\n",
        "    submission.to_csv('submission_pytorch.csv', index=False)\n",
        "    print(f\"PyTorch submission saved! Shape: {submission.shape}\")\n",
        "    return submission\n",
        "\n",
        "def generate_submission_tensorflow(model, test, test_input):\n",
        "    \"\"\"Generate submission for TensorFlow model using autoregressive inference\"\"\"\n",
        "    print(\"Generating TensorFlow submission...\")\n",
        "    sequences, metadata = prepare_test_sequences(test_input, max_inp=60)\n",
        "\n",
        "    # Use autoregressive inference (matching PyTorch)\n",
        "    predictions_all = model.predict_autoregressive(sequences)\n",
        "\n",
        "    pred_map = {}\n",
        "    for meta, pred in zip(metadata, predictions_all):\n",
        "        key = (meta['game_id'], meta['play_id'], meta['nfl_id'])\n",
        "        pred_map[key] = pred\n",
        "\n",
        "    results = []\n",
        "    for _, row in test.iterrows():\n",
        "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
        "        frame_id = row['frame_id']\n",
        "\n",
        "        if key in pred_map:\n",
        "            frame_idx = frame_id - 1\n",
        "            if 0 <= frame_idx < 40:\n",
        "                x_pred, y_pred = pred_map[key][frame_idx]\n",
        "            else:\n",
        "                x_pred, y_pred = pred_map[key][-1]\n",
        "        else:\n",
        "            x_pred, y_pred = 60.0, 26.65\n",
        "\n",
        "        x_pred = np.clip(x_pred, 0.0, 120.0)\n",
        "        y_pred = np.clip(y_pred, 0.0, 53.3)\n",
        "        results.append({'x': float(x_pred), 'y': float(y_pred)})\n",
        "\n",
        "    submission = pd.DataFrame(results)\n",
        "    submission.to_csv('submission_tensorflow.csv', index=False)\n",
        "    print(f\"TensorFlow submission saved! Shape: {submission.shape}\")\n",
        "    return submission\n",
        "\n",
        "# Generate all submissions\n",
        "print(\"\\nGenerating submissions for all models...\")\n",
        "\n",
        "# Load best models\n",
        "pytorch_model.load_state_dict(torch.load(\"best_model_pytorch.pt\", map_location=device))\n",
        "pytorch_model.eval()\n",
        "\n",
        "tf_model.load_weights('best_model_tensorflow.weights.h5')\n",
        "\n",
        "submission_pytorch = generate_submission_pytorch(pytorch_model, test, test_input)\n",
        "submission_tensorflow = generate_submission_tensorflow(tf_model, test, test_input)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL SUBMISSIONS GENERATED\")\n",
        "print(\"=\"*80)\n",
        "print(\"Files created:\")\n",
        "print(\"  - submission_pytorch.csv\")\n",
        "print(\"  - submission_tensorflow.csv\")\n",
        "print(\"  - model_comparison_results.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Model':<20} {'RMSE':<15} {'Submission File':<30}\")\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'PyTorch':<20} {results['PyTorch']:<15.4f} {'submission_pytorch.csv':<30}\")\n",
        "print(f\"{'TensorFlow':<20} {results['TensorFlow']:<15.4f} {'submission_tensorflow.csv':<30}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 1) Parameter count check\n",
        "# ============================\n",
        "\n",
        "def count_torch_params(model: torch.nn.Module) -> int:\n",
        "    # Count trainable parameters only\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def count_tf_params(model: tf.keras.Model) -> int:\n",
        "    # Keras count_params counts trainable + non-trainable by default,\n",
        "    # but in subclassed models it reflects built weights.\n",
        "    return model.count_params()\n",
        "\n",
        "print(\"PyTorch trainable params:\", f\"{count_torch_params(pytorch_model):,}\")\n",
        "print(\"TensorFlow params:\", f\"{count_tf_params(tf_model):,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOom9O-HUgSF",
        "outputId": "40ec6496-3757-41c4-dabe-1783891ca5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch trainable params: 1,587,202\n",
            "TensorFlow params: 1,587,202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 2A) PyTorch weight shapes\n",
        "# ============================\n",
        "\n",
        "def print_torch_gru_shapes(gru: torch.nn.GRU, prefix=\"gru\"):\n",
        "    # PyTorch GRU has parameters like:\n",
        "    # weight_ih_l{k}, weight_hh_l{k}, bias_ih_l{k}, bias_hh_l{k}\n",
        "    for name, p in gru.named_parameters():\n",
        "        print(f\"[Torch {prefix}] {name:20s} {tuple(p.shape)}\")\n",
        "\n",
        "print(\"=== PyTorch enc_embed ===\")\n",
        "print(\"weight:\", tuple(pytorch_model.enc_embed.weight.shape), \"bias:\", tuple(pytorch_model.enc_embed.bias.shape))\n",
        "\n",
        "print(\"\\n=== PyTorch enc_gru ===\")\n",
        "print_torch_gru_shapes(pytorch_model.enc_gru, prefix=\"enc_gru\")\n",
        "\n",
        "print(\"\\n=== PyTorch dec_embed ===\")\n",
        "print(\"weight:\", tuple(pytorch_model.dec_embed.weight.shape), \"bias:\", tuple(pytorch_model.dec_embed.bias.shape))\n",
        "\n",
        "print(\"\\n=== PyTorch dec_gru ===\")\n",
        "print_torch_gru_shapes(pytorch_model.dec_gru, prefix=\"dec_gru\")\n",
        "\n",
        "print(\"\\n=== PyTorch out_proj ===\")\n",
        "print(\"weight:\", tuple(pytorch_model.out_proj.weight.shape), \"bias:\", tuple(pytorch_model.out_proj.bias.shape))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4iYrNFnUiHr",
        "outputId": "abaf5533-2b4d-440b-b317-db21c8969ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PyTorch enc_embed ===\n",
            "weight: (256, 26) bias: (256,)\n",
            "\n",
            "=== PyTorch enc_gru ===\n",
            "[Torch enc_gru] weight_ih_l0         (768, 256)\n",
            "[Torch enc_gru] weight_hh_l0         (768, 256)\n",
            "[Torch enc_gru] bias_ih_l0           (768,)\n",
            "[Torch enc_gru] bias_hh_l0           (768,)\n",
            "[Torch enc_gru] weight_ih_l1         (768, 256)\n",
            "[Torch enc_gru] weight_hh_l1         (768, 256)\n",
            "[Torch enc_gru] bias_ih_l1           (768,)\n",
            "[Torch enc_gru] bias_hh_l1           (768,)\n",
            "\n",
            "=== PyTorch dec_embed ===\n",
            "weight: (256, 2) bias: (256,)\n",
            "\n",
            "=== PyTorch dec_gru ===\n",
            "[Torch dec_gru] weight_ih_l0         (768, 256)\n",
            "[Torch dec_gru] weight_hh_l0         (768, 256)\n",
            "[Torch dec_gru] bias_ih_l0           (768,)\n",
            "[Torch dec_gru] bias_hh_l0           (768,)\n",
            "[Torch dec_gru] weight_ih_l1         (768, 256)\n",
            "[Torch dec_gru] weight_hh_l1         (768, 256)\n",
            "[Torch dec_gru] bias_ih_l1           (768,)\n",
            "[Torch dec_gru] bias_hh_l1           (768,)\n",
            "\n",
            "=== PyTorch out_proj ===\n",
            "weight: (2, 256) bias: (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 2B) TensorFlow weight shapes\n",
        "# ============================\n",
        "\n",
        "def print_tf_layer_weights(layer: tf.keras.layers.Layer, prefix=\"layer\"):\n",
        "    weights = layer.weights\n",
        "    if not weights:\n",
        "        print(f\"[TF {prefix}] (no weights or not built)\")\n",
        "        return\n",
        "    for w in weights:\n",
        "        print(f\"[TF {prefix}] {w.name:45s} {tuple(w.shape)}\")\n",
        "\n",
        "print(\"=== TensorFlow enc_embed ===\")\n",
        "print_tf_layer_weights(tf_model.enc_embed, prefix=\"enc_embed\")\n",
        "\n",
        "print(\"\\n=== TensorFlow enc_gru (StackedRNNCells inside RNN) ===\")\n",
        "# Each GRUCell has kernel, recurrent_kernel, bias\n",
        "print_tf_layer_weights(tf_model.enc_gru_cell1, prefix=\"enc_gru_cell1\")\n",
        "print_tf_layer_weights(tf_model.enc_gru_cell2, prefix=\"enc_gru_cell2\")\n",
        "\n",
        "print(\"\\n=== TensorFlow dec_embed ===\")\n",
        "print_tf_layer_weights(tf_model.dec_embed, prefix=\"dec_embed\")\n",
        "\n",
        "print(\"\\n=== TensorFlow dec_gru (StackedRNNCells inside RNN) ===\")\n",
        "print_tf_layer_weights(tf_model.dec_gru_cell1, prefix=\"dec_gru_cell1\")\n",
        "print_tf_layer_weights(tf_model.dec_gru_cell2, prefix=\"dec_gru_cell2\")\n",
        "\n",
        "print(\"\\n=== TensorFlow out_proj ===\")\n",
        "print_tf_layer_weights(tf_model.out_proj, prefix=\"out_proj\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB7bPFk7UmC1",
        "outputId": "ef33c4a8-8f35-4621-d173-914d0322cc6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TensorFlow enc_embed ===\n",
            "[TF enc_embed] kernel                                        (26, 256)\n",
            "[TF enc_embed] bias                                          (256,)\n",
            "\n",
            "=== TensorFlow enc_gru (StackedRNNCells inside RNN) ===\n",
            "[TF enc_gru_cell1] kernel                                        (256, 768)\n",
            "[TF enc_gru_cell1] recurrent_kernel                              (256, 768)\n",
            "[TF enc_gru_cell1] bias                                          (2, 768)\n",
            "[TF enc_gru_cell2] kernel                                        (256, 768)\n",
            "[TF enc_gru_cell2] recurrent_kernel                              (256, 768)\n",
            "[TF enc_gru_cell2] bias                                          (2, 768)\n",
            "\n",
            "=== TensorFlow dec_embed ===\n",
            "[TF dec_embed] kernel                                        (2, 256)\n",
            "[TF dec_embed] bias                                          (256,)\n",
            "\n",
            "=== TensorFlow dec_gru (StackedRNNCells inside RNN) ===\n",
            "[TF dec_gru_cell1] kernel                                        (256, 768)\n",
            "[TF dec_gru_cell1] recurrent_kernel                              (256, 768)\n",
            "[TF dec_gru_cell1] bias                                          (2, 768)\n",
            "[TF dec_gru_cell2] kernel                                        (256, 768)\n",
            "[TF dec_gru_cell2] recurrent_kernel                              (256, 768)\n",
            "[TF dec_gru_cell2] bias                                          (2, 768)\n",
            "\n",
            "=== TensorFlow out_proj ===\n",
            "[TF out_proj] kernel                                        (256, 2)\n",
            "[TF out_proj] bias                                          (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 3) Forward output shape check\n",
        "# ============================\n",
        "\n",
        "# Dummy inputs\n",
        "B = 4\n",
        "T_in = 60\n",
        "T_out = 40\n",
        "D = len(FEATURES)\n",
        "\n",
        "x_np = np.random.randn(B, T_in, D).astype(\"float32\")\n",
        "y_np = np.random.randn(B, T_out, 2).astype(\"float32\")\n",
        "\n",
        "# PyTorch: teacher forcing forward\n",
        "pytorch_model.train()\n",
        "with torch.no_grad():\n",
        "    x_t = torch.from_numpy(x_np).to(device)\n",
        "    y_t = torch.from_numpy(y_np).to(device)\n",
        "    out_torch_train = pytorch_model(x_t, target=y_t).cpu().numpy()\n",
        "\n",
        "# PyTorch: autoregressive forward\n",
        "pytorch_model.eval()\n",
        "with torch.no_grad():\n",
        "    out_torch_inf = pytorch_model(x_t).cpu().numpy()\n",
        "\n",
        "# TF: teacher forcing forward\n",
        "tf_out_train = tf_model([x_np, y_np], training=True).numpy()\n",
        "\n",
        "# TF: autoregressive forward\n",
        "tf_out_inf = tf_model.predict_autoregressive(x_np)\n",
        "\n",
        "print(\"Torch train out shape:\", out_torch_train.shape)\n",
        "print(\"TF    train out shape:\", tf_out_train.shape)\n",
        "\n",
        "print(\"Torch infer out shape:\", out_torch_inf.shape)\n",
        "print(\"TF    infer out shape:\", tf_out_inf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8Yz6LMrUwcA",
        "outputId": "ce555d0a-b2eb-49bb-b514-9cc201b8fb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch train out shape: (4, 40, 2)\n",
            "TF    train out shape: (4, 40, 2)\n",
            "Torch infer out shape: (4, 40, 2)\n",
            "TF    infer out shape: (4, 40, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "def compare_submissions(\n",
        "    path_pt=\"submission_pytorch.csv\",\n",
        "    path_tf=\"submission_tensorflow.csv\",\n",
        "    eps=1e-12\n",
        "):\n",
        "    # -----------------------\n",
        "    # 1) Load + sanity checks\n",
        "    # -----------------------\n",
        "    pt = pd.read_csv(path_pt)\n",
        "    tf = pd.read_csv(path_tf)\n",
        "\n",
        "    required_cols = {\"x\", \"y\"}\n",
        "    if not required_cols.issubset(pt.columns) or not required_cols.issubset(tf.columns):\n",
        "        raise ValueError(f\"Both files must contain columns {required_cols}\")\n",
        "\n",
        "    if len(pt) != len(tf):\n",
        "        raise ValueError(f\"Row count mismatch: PyTorch={len(pt)}, TensorFlow={len(tf)}\")\n",
        "\n",
        "    # ensure numeric\n",
        "    pt = pt[[\"x\", \"y\"]].astype(float).reset_index(drop=True)\n",
        "    tf = tf[[\"x\", \"y\"]].astype(float).reset_index(drop=True)\n",
        "\n",
        "    # -----------------------\n",
        "    # 2) Row-wise differences\n",
        "    # -----------------------\n",
        "    dx = tf[\"x\"].values - pt[\"x\"].values\n",
        "    dy = tf[\"y\"].values - pt[\"y\"].values\n",
        "    dist = np.sqrt(dx**2 + dy**2)  # Euclidean distance between model predictions\n",
        "\n",
        "    # PyTorch-as-reference error metrics (NOT ground-truth)\n",
        "    rmse_x = np.sqrt(np.mean(dx**2))\n",
        "    rmse_y = np.sqrt(np.mean(dy**2))\n",
        "    rmse_xy = np.sqrt(np.mean(dx**2 + dy**2))  # RMSE in 2D\n",
        "    mae_x = np.mean(np.abs(dx))\n",
        "    mae_y = np.mean(np.abs(dy))\n",
        "    mae_xy = np.mean(dist)\n",
        "\n",
        "    # Correlation + R^2 (per coordinate)\n",
        "    # (R^2 here is \"how well TF explains PT\" linearly; still not accuracy vs GT)\n",
        "    def r2(y_true, y_pred):\n",
        "        ss_res = np.sum((y_true - y_pred)**2)\n",
        "        ss_tot = np.sum((y_true - np.mean(y_true))**2) + eps\n",
        "        return 1.0 - ss_res / ss_tot\n",
        "\n",
        "    corr_x = np.corrcoef(pt[\"x\"].values, tf[\"x\"].values)[0, 1]\n",
        "    corr_y = np.corrcoef(pt[\"y\"].values, tf[\"y\"].values)[0, 1]\n",
        "    r2_x = r2(pt[\"x\"].values, tf[\"x\"].values)\n",
        "    r2_y = r2(pt[\"y\"].values, tf[\"y\"].values)\n",
        "\n",
        "    # -----------------------\n",
        "    # 3) Statistical tests\n",
        "    # -----------------------\n",
        "    # Paired tests on differences (H0: mean/median difference = 0)\n",
        "    t_x = stats.ttest_rel(tf[\"x\"].values, pt[\"x\"].values, nan_policy=\"omit\")\n",
        "    t_y = stats.ttest_rel(tf[\"y\"].values, pt[\"y\"].values, nan_policy=\"omit\")\n",
        "\n",
        "    # Wilcoxon signed-rank (robust alternative); requires non-all-zero diffs\n",
        "    def safe_wilcoxon(d):\n",
        "        d = d[np.isfinite(d)]\n",
        "        if np.allclose(d, 0.0):\n",
        "            return None\n",
        "        return stats.wilcoxon(d)\n",
        "\n",
        "    w_x = safe_wilcoxon(dx)\n",
        "    w_y = safe_wilcoxon(dy)\n",
        "\n",
        "    # KS test on marginal distributions (H0: same distribution)\n",
        "    ks_x = stats.ks_2samp(pt[\"x\"].values, tf[\"x\"].values)\n",
        "    ks_y = stats.ks_2samp(pt[\"y\"].values, tf[\"y\"].values)\n",
        "\n",
        "    # Effect size: Cohen's d for paired differences\n",
        "    def cohens_d_paired(d):\n",
        "        d = d[np.isfinite(d)]\n",
        "        return np.mean(d) / (np.std(d, ddof=1) + eps)\n",
        "\n",
        "    d_x = cohens_d_paired(dx)\n",
        "    d_y = cohens_d_paired(dy)\n",
        "\n",
        "    # -----------------------\n",
        "    # 4) Summaries + outliers\n",
        "    # -----------------------\n",
        "    summary = {\n",
        "        \"n_rows\": len(pt),\n",
        "\n",
        "        \"diff_mean_x (TF-PT)\": float(np.mean(dx)),\n",
        "        \"diff_std_x\": float(np.std(dx, ddof=1)),\n",
        "        \"diff_median_x\": float(np.median(dx)),\n",
        "\n",
        "        \"diff_mean_y (TF-PT)\": float(np.mean(dy)),\n",
        "        \"diff_std_y\": float(np.std(dy, ddof=1)),\n",
        "        \"diff_median_y\": float(np.median(dy)),\n",
        "\n",
        "        \"dist_mean\": float(np.mean(dist)),\n",
        "        \"dist_median\": float(np.median(dist)),\n",
        "        \"dist_p95\": float(np.quantile(dist, 0.95)),\n",
        "        \"dist_p99\": float(np.quantile(dist, 0.99)),\n",
        "        \"dist_max\": float(np.max(dist)),\n",
        "\n",
        "        \"rmse_x (TF vs PT)\": float(rmse_x),\n",
        "        \"rmse_y (TF vs PT)\": float(rmse_y),\n",
        "        \"rmse_xy (2D)\": float(rmse_xy),\n",
        "        \"mae_x (TF vs PT)\": float(mae_x),\n",
        "        \"mae_y (TF vs PT)\": float(mae_y),\n",
        "        \"mae_xy (2D)\": float(mae_xy),\n",
        "\n",
        "        \"corr_x (PT vs TF)\": float(corr_x),\n",
        "        \"corr_y (PT vs TF)\": float(corr_y),\n",
        "        \"r2_x (PT~TF)\": float(r2_x),\n",
        "        \"r2_y (PT~TF)\": float(r2_y),\n",
        "\n",
        "        \"paired_ttest_x_stat\": float(t_x.statistic),\n",
        "        \"paired_ttest_x_p\": float(t_x.pvalue),\n",
        "        \"paired_ttest_y_stat\": float(t_y.statistic),\n",
        "        \"paired_ttest_y_p\": float(t_y.pvalue),\n",
        "\n",
        "        \"wilcoxon_x_stat\": None if w_x is None else float(w_x.statistic),\n",
        "        \"wilcoxon_x_p\": None if w_x is None else float(w_x.pvalue),\n",
        "        \"wilcoxon_y_stat\": None if w_y is None else float(w_y.statistic),\n",
        "        \"wilcoxon_y_p\": None if w_y is None else float(w_y.pvalue),\n",
        "\n",
        "        \"ks_x_stat\": float(ks_x.statistic),\n",
        "        \"ks_x_p\": float(ks_x.pvalue),\n",
        "        \"ks_y_stat\": float(ks_y.statistic),\n",
        "        \"ks_y_p\": float(ks_y.pvalue),\n",
        "\n",
        "        \"cohens_d_paired_x\": float(d_x),\n",
        "        \"cohens_d_paired_y\": float(d_y),\n",
        "    }\n",
        "\n",
        "    # Outliers: top 1% distances\n",
        "    k = max(1, int(0.01 * len(dist)))\n",
        "    top_idx = np.argsort(dist)[-k:][::-1]\n",
        "    outliers = pd.DataFrame({\n",
        "        \"row_index\": top_idx,\n",
        "        \"pt_x\": pt.loc[top_idx, \"x\"].values,\n",
        "        \"pt_y\": pt.loc[top_idx, \"y\"].values,\n",
        "        \"tf_x\": tf.loc[top_idx, \"x\"].values,\n",
        "        \"tf_y\": tf.loc[top_idx, \"y\"].values,\n",
        "        \"dx\": dx[top_idx],\n",
        "        \"dy\": dy[top_idx],\n",
        "        \"dist\": dist[top_idx],\n",
        "    })\n",
        "\n",
        "    # Print a clean report\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUBMISSION COMPARISON (TensorFlow vs PyTorch; paired by row index)\")\n",
        "    print(\"=\"*80)\n",
        "    for k, v in summary.items():\n",
        "        print(f\"{k:<28}: {v}\")\n",
        "\n",
        "    print(\"\\nTop 1% largest per-row prediction gaps (Euclidean distance):\")\n",
        "    print(outliers.head(20).to_string(index=False))\n",
        "\n",
        "    return summary, outliers\n",
        "\n",
        "# Run\n",
        "summary, outliers = compare_submissions(\n",
        "    \"submission_pytorch.csv\",\n",
        "    \"submission_tensorflow.csv\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llATCL_RVc59",
        "outputId": "33e85ae7-9e7b-4e29-953a-3f74d3b67855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SUBMISSION COMPARISON (TensorFlow vs PyTorch; paired by row index)\n",
            "================================================================================\n",
            "n_rows                      : 5837\n",
            "diff_mean_x (TF-PT)         : -0.3924265122687304\n",
            "diff_std_x                  : 23.550622916874378\n",
            "diff_median_x               : -2.234142303466797\n",
            "diff_mean_y (TF-PT)         : -1.1467200008967529\n",
            "diff_std_y                  : 11.350704544439992\n",
            "diff_median_y               : -1.7814788818359375\n",
            "dist_mean                   : 22.651423285753275\n",
            "dist_median                 : 20.104820931358756\n",
            "dist_p95                    : 48.510847327525546\n",
            "dist_p99                    : 60.93940527707063\n",
            "dist_max                    : 79.32014351923331\n",
            "rmse_x (TF vs PT)           : 23.55187504899019\n",
            "rmse_y (TF vs PT)           : 11.407514527319222\n",
            "rmse_xy (2D)                : 26.16910785667417\n",
            "mae_x (TF vs PT)            : 18.780578349170487\n",
            "mae_y (TF vs PT)            : 9.746711684346547\n",
            "mae_xy (2D)                 : 22.651423285753275\n",
            "corr_x (PT vs TF)           : 0.27247461804061796\n",
            "corr_y (PT vs TF)           : 0.4953156828693493\n",
            "r2_x (PT~TF)                : 0.07080717299609796\n",
            "r2_y (PT~TF)                : 0.2213817032575207\n",
            "paired_ttest_x_stat         : -1.273065615819711\n",
            "paired_ttest_x_p            : 0.20304546769146908\n",
            "paired_ttest_y_stat         : -7.718437909600432\n",
            "paired_ttest_y_p            : 1.3765228630025133e-14\n",
            "wilcoxon_x_stat             : 8245415.5\n",
            "wilcoxon_x_p                : 0.033527882360297516\n",
            "wilcoxon_y_stat             : 7561560.0\n",
            "wilcoxon_y_p                : 1.0286272797309157e-13\n",
            "ks_x_stat                   : 0.41562446462223746\n",
            "ks_x_p                      : 0.0\n",
            "ks_y_stat                   : 0.408771629261607\n",
            "ks_y_p                      : 0.0\n",
            "cohens_d_paired_x           : -0.016663105415676043\n",
            "cohens_d_paired_y           : -0.1010263280492451\n",
            "\n",
            "Top 1% largest per-row prediction gaps (Euclidean distance):\n",
            " row_index       pt_x      pt_y      tf_x      tf_y         dx        dy      dist\n",
            "      4988 114.877449  5.222170 37.987812 24.707264 -76.889637 19.485094 79.320144\n",
            "      4969 113.960548  4.467256 37.810516 25.041595 -76.150032 20.574340 78.880485\n",
            "      4987 115.017105  5.352698 38.858994 24.127243 -76.158112 18.774545 78.438138\n",
            "      4968 114.128784  4.611125 38.689682 24.449833 -75.439102 19.838708 78.004054\n",
            "      4986 115.164650  5.494499 39.820713 23.588018 -75.343937 18.093520 77.486026\n",
            "      4967 114.280403  4.755975 39.667236 23.898319 -74.613167 19.142344 77.029566\n",
            "      4985 115.314209  5.644044 40.863983 23.092443 -74.450226 17.448399 76.467527\n",
            "      4966 114.398689  4.933280 40.728073 23.394104 -73.670616 18.460824 75.948415\n",
            "      4984 115.434410  5.795792 41.985497 22.644217 -73.448914 16.848424 75.356568\n",
            "      4965 114.403023  5.279089 41.874130 22.940659 -72.528893 17.661570 74.648317\n",
            "      4983 115.487434  6.063587 43.196854 22.231226 -72.290581 16.167639 74.076451\n",
            "      4964 114.227562  5.870659 43.107292 22.528347 -71.120270 16.657688 73.044995\n",
            "      4982 115.310951  6.586613 44.530693 21.853451 -70.780258 15.266838 72.408019\n",
            "      4963 114.012199  6.498855 44.458698 22.154556 -69.553501 15.655701 71.293692\n",
            "      4981 115.016953  7.228470 46.045513 21.506247 -68.971439 14.277777 70.433759\n",
            "      4962 113.813713  7.137012 45.987377 21.812237 -67.826336 14.675225 69.395779\n",
            "      5626 103.782364 32.761761 34.521355 33.469616 -69.261009  0.707855 69.264626\n",
            "      4980 114.717613  7.803994 47.752533 21.182606 -66.965080 13.378612 68.288427\n",
            "      5625 102.810219 32.511791 34.599762 32.451347 -68.210457 -0.060444 68.210484\n",
            "      4961 113.572258  7.791035 47.712307 21.491379 -65.859951 13.700344 67.269849\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e17f0a1c1a0430181b65f5897634c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc753480e95a433eb5e08dda444ea760",
              "IPY_MODEL_a6d96fb63ae94287b8d36c1934a74e29",
              "IPY_MODEL_7ae3dd01b2904ec2b558863267f97b93",
              "IPY_MODEL_99e36565df8b40249b75030702fd23e7",
              "IPY_MODEL_e440ccd6cbbb43b8b8c52d3976b0e74c"
            ],
            "layout": "IPY_MODEL_4060f7a1e89a4b689c6013146f088b18"
          }
        },
        "fc753480e95a433eb5e08dda444ea760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da5f7b10d55f4257bfab1810a58412ca",
            "placeholder": "​",
            "style": "IPY_MODEL_af35f637089d46e1a42c9dcae9422503",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "a6d96fb63ae94287b8d36c1934a74e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_828a5316430d483e800d1569dd6ff41d",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f34a152cff4e62807288fb1a4007f7",
            "value": ""
          }
        },
        "7ae3dd01b2904ec2b558863267f97b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_66e7222406dd4aaca27014ffa02f8b6b",
            "placeholder": "​",
            "style": "IPY_MODEL_49a15e892c0642c7bb14d15f88871d97",
            "value": ""
          }
        },
        "99e36565df8b40249b75030702fd23e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_de5994b7a60c4932b1f2c502c11fb013",
            "style": "IPY_MODEL_6a44df16a9e04dc8a9a091a65128d47b",
            "tooltip": ""
          }
        },
        "e440ccd6cbbb43b8b8c52d3976b0e74c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_056414beb68744df9bb875616146a0d8",
            "placeholder": "​",
            "style": "IPY_MODEL_6cebe472cf3b46a181c41036020e5b81",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "4060f7a1e89a4b689c6013146f088b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "da5f7b10d55f4257bfab1810a58412ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af35f637089d46e1a42c9dcae9422503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "828a5316430d483e800d1569dd6ff41d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f34a152cff4e62807288fb1a4007f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66e7222406dd4aaca27014ffa02f8b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a15e892c0642c7bb14d15f88871d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de5994b7a60c4932b1f2c502c11fb013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a44df16a9e04dc8a9a091a65128d47b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "056414beb68744df9bb875616146a0d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cebe472cf3b46a181c41036020e5b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}